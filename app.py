import streamlit as st
import openai as op
import pyaudio
import wave
import os
import time
import json
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from threading import Thread
import pandas as pd
import numpy as np
from gtts import gTTS
from sklearn.linear_model import LinearRegression
import random
import io
import PyPDF2
from docx import Document
import matplotlib.pyplot as plt
from transformers import pipeline
import re

# Configura√ß√£o das APIs
op.api_key = os.getenv('OPENAI_API_KEY', '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')
GOOGLE_API_KEY = "AIzaSyDIUhxzxID7vmlnzzRCT4Qdu9cFKGO0dcw"  # Substitua por sua chave real
GOOGLE_CX = "76d77ecfde40f4253"  # Substitua por seu CX real

# Sistema de auto-atualiza√ß√£o e pesquisa
class AutonomousAssistant:
    def __init__(self):
        # Inicializar vari√°veis de estado primeiro
        self.current_thought = "Inicializando sistema..."
        self.current_progress = 0
        
        # Depois inicializar outros componentes
        self.knowledge_file = "knowledge_base.json"
        self.update_log = "update_log.txt"
        self.load_knowledge()
        self.search_history = []
        self.performance_metrics = {
            "response_times": [],
            "accuracy_scores": [],
            "user_ratings": []
        }
        self.scheduled_tasks = []
        self.load_tasks()
        
        # Novos sistemas
        self.conversation_memory = []
        self.important_facts = {}
        self.plugins = self.load_plugins()
        self.init_collaboration()
        
    def update_thought(self, message, progress_increment=0):
        """Atualiza o pensamento atual e o progresso"""
        self.current_thought = message
        if progress_increment:
            self.current_progress = min(100, self.current_progress + progress_increment)
        
        # Atualizar a interface se estiver em execu√ß√£o
        if 'thought_ui' in st.session_state:
            st.session_state.thought_ui.markdown(f"üí≠ **Pensando:** {self.current_thought}")
        if 'progress_ui' in st.session_state:
            st.session_state.progress_ui.progress(self.current_progress)
        
        time.sleep(0.5)  # Dar tempo para a UI atualizar
        
    def load_knowledge(self):
        self.update_thought("Carregando base de conhecimento...", 5)
        try:
            if os.path.exists(self.knowledge_file):
                with open(self.knowledge_file, 'r') as f:
                    self.knowledge = json.load(f)
            else:
                self.knowledge = {
                    "expertise": {
                        "python": 9,
                        "web_development": 8,
                        "data_science": 8,
                        "ai_research": 8,
                        "voice_processing": 7,
                        "auto_optimization": 6,
                        "document_analysis": 5,
                        "business_intelligence": 6
                    },
                    "behavior": {
                        "response_style": "analytical",
                        "detail_level": "comprehensive",
                        "autonomy_level": "high",
                        "response_speed": 1.0,
                        "research_depth": 2,
                        "memory_context": 3
                    },
                    "updates": []
                }
        except Exception as e:
            self.knowledge = {
                "expertise": {
                    "python": 9,
                    "web_development": 8,
                    "data_science": 8,
                    "ai_research": 8,
                    "voice_processing": 7,
                    "auto_optimization": 6,
                    "document_analysis": 5,
                    "business_intelligence": 6
                },
                "behavior": {
                    "response_style": "analytical",
                    "detail_level": "comprehensive",
                    "autonomy_level": "high",
                    "response_speed": 1.0,
                    "research_depth": 2,
                    "memory_context": 3
                },
                "updates": []
            }
        self.update_thought("Base de conhecimento carregada", 5)
    
    def save_knowledge(self):
        self.update_thought("Salvando conhecimento atualizado...", 5)
        with open(self.knowledge_file, 'w') as f:
            json.dump(self.knowledge, f, indent=2)
        self.update_thought("Conhecimento salvo com sucesso", 5)
    
    def log_update(self, update):
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"[{timestamp}] {update}\n"
        self.knowledge['updates'].append(log_entry)
        
        with open(self.update_log, 'a') as f:
            f.write(log_entry)
        
        self.save_knowledge()
        self.update_thought(f"Atualiza√ß√£o registrada: {update}", 3)
    
    def load_tasks(self):
        self.update_thought("Carregando tarefas agendadas...", 5)
        try:
            if os.path.exists("scheduled_tasks.json"):
                with open("scheduled_tasks.json", 'r') as f:
                    self.scheduled_tasks = json.load(f)
        except:
            self.scheduled_tasks = []
        self.update_thought(f"{len(self.scheduled_tasks)} tarefas carregadas", 5)
    
    def save_tasks(self):
        self.update_thought("Salvando tarefas agendadas...", 5)
        with open("scheduled_tasks.json", 'w') as f:
            json.dump(self.scheduled_tasks, f, indent=2)
        self.update_thought("Tarefas salvas com sucesso", 5)
    
    def should_research(self, question):
        self.update_thought("Analisando necessidade de pesquisa...", 10)
        decision_prompt = f"""
        Com base na pergunta abaixo, decida se √© necess√°rio pesquisar informa√ß√µes atualizadas na internet.
        Responda apenas com 'SIM' ou 'N√ÉO', sem explica√ß√µes.
        
        Pergunta: {question}
        """
        
        response = op.ChatCompletion.create(
            model="gpt-4",
            messages=[{"role": "user", "content": decision_prompt}],
            max_tokens=10,
            temperature=0.0
        )
        decision = "SIM" in response.choices[0].message['content'].strip().upper()
        
        if decision:
            self.update_thought("Pesquisa na web necess√°ria", 5)
        else:
            self.update_thought("Usando conhecimento interno", 5)
            
        return decision
    
    def web_search(self, query):
        self.update_thought(f"Pesquisando: '{query}'", 10)
        try:
            # Pesquisa usando API do Google
            search_url = f"https://www.googleapis.com/customsearch/v1?key={GOOGLE_API_KEY}&cx={GOOGLE_CX}&q={requests.utils.quote(query)}"
            response = requests.get(search_url)
            response.raise_for_status()
            data = response.json()
            
            # Processar resultados
            results = []
            for item in data.get('items', [])[:5]:  # Limitar a 5 resultados
                results.append({
                    "title": item.get('title'),
                    "url": item.get('link'),
                    "snippet": item.get('snippet', '')[:200] + '...'  # Limitar snippet
                })
            
            self.search_history.append({
                "query": query,
                "results": results,
                "timestamp": datetime.now().isoformat(),
                "source": "Google API"
            })
            
            self.update_thought(f"{len(results)} resultados encontrados", 10)
            return results
        except Exception as e:
            self.update_thought(f"Erro na pesquisa: {str(e)}", 0)
            st.error(f"Erro na pesquisa: {str(e)}")
            return []
    
    # ========== NOVAS FUNCIONALIDADES ==========
    
    def remember_context(self, conversation, important_facts):
        """Armazena contexto importante da conversa"""
        self.conversation_memory.append({
            "timestamp": datetime.now().isoformat(),
            "conversation": conversation
        })
        self.important_facts.update(important_facts)
        
        # Limitar mem√≥ria a 10 itens
        if len(self.conversation_memory) > 10:
            self.conversation_memory.pop(0)
            
        self.log_update("Contexto atualizado na mem√≥ria")
    
    def recall_context(self):
        """Recupera o contexto relevante"""
        if not self.conversation_memory:
            return "Nenhum contexto anterior dispon√≠vel"
        
        context = "Contexto da conversa:\n"
        for i, mem in enumerate(self.conversation_memory[-3:]):
            context += f"{i+1}. {mem['conversation']}\n"
            
        if self.important_facts:
            context += "\nFatos importantes:\n"
            for key, value in self.important_facts.items():
                context += f"- {key}: {value}\n"
                
        return context
    
    def process_document(self, file):
        """Processa documentos PDF, Word e TXT"""
        self.update_thought(f"Processando documento: {file.name}", 10)
        content = ""
        
        try:
            # PDF
            if file.type == "application/pdf":
                reader = PyPDF2.PdfReader(file)
                for page in reader.pages:
                    content += page.extract_text() + "\n"
                    
            # Word
            elif file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
                doc = Document(io.BytesIO(file.getvalue()))
                for para in doc.paragraphs:
                    content += para.text + "\n"
                    
            # Texto
            elif file.type == "text/plain":
                content = file.getvalue().decode("utf-8")
                
            else:
                return None, "Tipo de documento n√£o suportado"
                
            # Resumir documento
            summary_prompt = f"""
            Resuma o documento abaixo para incluir apenas informa√ß√µes essenciais.
            Mantenha dados t√©cnicos, n√∫meros importantes e conclus√µes.
            
            Documento:
            {content[:5000]}... [continua]
            """
            
            summary = op.ChatCompletion.create(
                model="gpt-4",
                messages=[{"role": "user", "content": summary_prompt}],
                max_tokens=800,
                temperature=0.3
            ).choices[0].message['content'].strip()
            
            # Extrair fatos importantes
            facts_prompt = f"""
            Extraia os fatos mais importantes do documento como um JSON:
            {{
                "titulo": "T√≠tulo do Documento",
                "autor": "Autor se dispon√≠vel",
                "data": "Data se dispon√≠vel",
                "pontos_chave": ["ponto 1", "ponto 2", ...],
                "decisoes": ["decis√£o 1", ...],
                "recomendacoes": ["recomenda√ß√£o 1", ...]
            }}
            """
            
            facts = op.ChatCompletion.create(
                model="gpt-4",
                messages=[{"role": "user", "content": facts_prompt}],
                max_tokens=500,
                temperature=0.1
            ).choices[0].message['content'].strip()
            
            return summary, facts
            
        except Exception as e:
            return None, f"Erro no processamento: {str(e)}"
    
    def business_intelligence_dashboard(self):
        """Dashboard de Business Intelligence"""
        if not self.performance_metrics['response_times']:
            return "Dados insuficientes para an√°lise"
            
        # Preparar dados
        metrics_df = pd.DataFrame({
            "Tempo Resposta": self.performance_metrics['response_times'],
            "Acur√°cia": self.performance_metrics['accuracy_scores'],
            "Avalia√ß√£o": self.performance_metrics['user_ratings']
        })
        
        # An√°lise preditiva
        X = np.array(metrics_df['Tempo Resposta']).reshape(-1, 1)
        y = np.array(metrics_df['Avalia√ß√£o'])
        
        if len(y) > 1:
            try:
                model = LinearRegression()
                model.fit(X, y)
                future_times = np.array([10, 15, 20]).reshape(-1, 1)
                predictions = model.predict(future_times)
                
                # Criar relat√≥rio
                report = f"""
                ## üìà Relat√≥rio de Performance
                
                **M√©tricas Atuais:**
                - Tempo m√©dio de resposta: {np.mean(metrics_df['Tempo Resposta']):.2f}s
                - Acur√°cia m√©dia: {np.mean(metrics_df['Acur√°cia'])*100:.1f}%
                - Avalia√ß√£o m√©dia do usu√°rio: {np.mean(metrics_df['Avalia√ß√£o']):.1f}/5
                
                **Previs√µes:**
                - Avalia√ß√£o estimada para 10s: {predictions[0]:.1f}
                - Avalia√ß√£o estimada para 15s: {predictions[1]:.1f}
                - Avalia√ß√£o estimada para 20s: {predictions[2]:.1f}
                
                **Recomenda√ß√µes:**
                - Otimizar processos para tempo m√©dio de 15s
                - Focar em melhorar precis√£o t√©cnica
                """
                
                # Gr√°fico
                fig, ax = plt.subplots()
                ax.scatter(metrics_df['Tempo Resposta'], metrics_df['Avalia√ß√£o'], color='blue')
                ax.plot(future_times, predictions, color='red', linewidth=2)
                ax.set_title('Rela√ß√£o Tempo de Resposta vs Avalia√ß√£o')
                ax.set_xlabel('Tempo de Resposta (s)')
                ax.set_ylabel('Avalia√ß√£o do Usu√°rio (1-5)')
                
                return report, fig
                
            except Exception as e:
                return "Erro na an√°lise preditiva", None
                
        return "Dados insuficientes para predi√ß√£o", None
    
    def analyze_sentiment(self, audio_file):
        """Analisa sentimento a partir de √°udio"""
        try:
            # Usando modelo pr√©-treinado
            classifier = pipeline("audio-classification", model="superb/hubert-large-superb-er")
            result = classifier(audio_file)
            sentiment = max(result, key=lambda x: x['score'])['label']
            
            # Traduzir para portugu√™s
            sentiment_map = {
                "angry": "raiva",
                "happy": "felicidade",
                "sad": "tristeza",
                "neutral": "neutro",
                "fear": "medo",
                "disgust": "desgosto",
                "surprise": "surpresa"
            }
            
            return sentiment_map.get(sentiment, sentiment)
            
        except Exception as e:
            return f"Erro na an√°lise: {str(e)}"
    
    def load_plugins(self):
        """Carrega plugins dispon√≠veis"""
        return {
            "stock": {
                "function": self.get_stock_data,
                "description": "Obter dados de a√ß√µes em tempo real"
            },
            "weather": {
                "function": self.get_weather,
                "description": "Previs√£o do tempo para uma localiza√ß√£o"
            }
        }
    
    def execute_plugin(self, plugin_name, params):
        """Executa um plugin espec√≠fico"""
        if plugin_name in self.plugins:
            try:
                return self.plugins[plugin_name]["function"](**params)
            except Exception as e:
                return f"Erro na execu√ß√£o: {str(e)}"
        return "Plugin n√£o encontrado"
    
    def get_stock_data(self, symbol):
        """Obt√©m dados de a√ß√µes (exemplo)"""
        # Implementa√ß√£o real usaria API como Alpha Vantage
        prices = {
            "AAPL": 185.25,
            "GOOGL": 138.42,
            "MSFT": 340.11,
            "AMZN": 145.18
        }
        return f"Cota√ß√£o {symbol}: ${prices.get(symbol, 'N/A')}"
    
    def get_weather(self, location):
        """Obt√©m previs√£o do tempo (exemplo)"""
        # Implementa√ß√£o real usaria API como OpenWeather
        forecasts = {
            "S√£o Paulo": "25¬∞C, Parcialmente nublado",
            "Rio de Janeiro": "28¬∞C, Ensolarado",
            "Bras√≠lia": "27¬∞C, Chuvas esparsas"
        }
        return f"Previs√£o em {location}: {forecasts.get(location, 'N/A')}"
    
    def init_collaboration(self):
        """Inicializa sistema de colabora√ß√£o"""
        if "collaboration" not in st.session_state:
            st.session_state.collaboration = {
                "users": [],
                "messages": [],
                "shared_files": []
            }
    
    def add_collaborator(self, user_id):
        """Adiciona colaborador √† sess√£o"""
        if user_id not in st.session_state.collaboration["users"]:
            st.session_state.collaboration["users"].append(user_id)
            return True
        return False
    
    def add_collaboration_message(self, user, message):
        """Adiciona mensagem ao chat colaborativo"""
        st.session_state.collaboration["messages"].append({
            "user": user,
            "message": message,
            "timestamp": datetime.now().strftime("%H:%M:%S")
        })
    
    def share_file(self, user, file_info):
        """Compartilha arquivo na sess√£o colaborativa"""
        st.session_state.collaboration["shared_files"].append({
            "user": user,
            "file": file_info,
            "timestamp": datetime.now().strftime("%H:%M:%S")
        })
    
    def continuous_learning(self):
        """Sistema de aprendizado cont√≠nuo"""
        if len(self.performance_metrics['accuracy_scores']) < 10:
            return "Dados insuficientes para aprendizado"
            
        # Identificar √°reas de melhoria
        weak_areas = []
        for area, score in self.knowledge['expertise'].items():
            if score < 7:
                weak_areas.append(area)
        
        if not weak_areas:
            return "Nenhuma √°rea de melhoria identificada"
        
        # Criar plano de estudo
        study_plan = f"Plano de Estudo Autom√°tico:\n"
        for area in weak_areas[:3]:
            study_plan += f"- Pesquisar avan√ßos recentes em {area}\n"
            study_plan += f"- Estudar pr√°ticas recomendadas para {area}\n"
            study_plan += f"- Analisar casos de uso em {area}\n\n"
            
            # Agendar tarefa de aprendizado
            new_task = {
                "name": f"Aprendizado: {area}",
                "type": "research",
                "query": f"avan√ßos recentes em {area} melhores pr√°ticas",
                "topic": area,
                "frequency": "once",
                "next_run": datetime.now().strftime("%Y-%m-%d %H:%M")
            }
            self.scheduled_tasks.append(new_task)
        
        self.save_tasks()
        return study_plan
    
    # ========== FIM DAS NOVAS FUNCIONALIDADES ==========
    
    def analyze_and_decide(self, question):
        self.current_progress = 0
        self.current_thought = "Iniciando processamento..."
        self.update_thought(self.current_thought, 0)
        
        # Recuperar contexto
        context = self.recall_context()
        
        # Etapa 1: An√°lise da pergunta
        self.update_thought("Analisando sua pergunta...", 10)
        analysis_prompt = f"""
        Como um assistente aut√¥nomo, analise a pergunta do usu√°rio e determine:
        1. O conhecimento necess√°rio para responder
        2. Se pesquisa na internet √© necess√°ria
        3. O n√≠vel de detalhe apropriado
        4. A melhor estrat√©gia de resposta
        
        Contexto anterior:
        {context}
        
        Pergunta: {question}
        
        Forne√ßa sua an√°lise em no m√°ximo 3 frases.
        """
        
        analysis = op.ChatCompletion.create(
            model="gpt-4",
            messages=[{"role": "user", "content": analysis_prompt}],
            max_tokens=200,
            temperature=0.3
        ).choices[0].message['content'].strip()
        
        # Etapa 2: Decis√£o de pesquisa
        research_results = []
        if self.should_research(question):
            self.update_thought("Criando query de pesquisa...", 10)
            research_query_prompt = f"""
            Com base na pergunta e an√°lise, gere uma query de pesquisa otimizada para o Google.
            Seja conciso e direto ao ponto.
            
            Pergunta: {question}
            An√°lise: {analysis}
            
            Forne√ßa apenas a query, sem texto adicional.
            """
            
            research_query = op.ChatCompletion.create(
                model="gpt-4",
                messages=[{"role": "user", "content": research_query_prompt}],
                max_tokens=40,
                temperature=0.2
            ).choices[0].message['content'].strip()
            
            research_results = self.web_search(research_query)
        
        # Etapa 3: S√≠ntese da resposta
        self.update_thought("Sintetizando resposta...", 15)
        research_context = ""
        if research_results:
            research_context = "\nDados de pesquisa:\n"
            for i, res in enumerate(research_results):
                research_context += f"{i+1}. [{res['title']}]({res['url']}): {res['snippet']}\n"
        
        synthesis_prompt = f"""
        Com base na an√°lise, {f"dados de pesquisa" if research_results else "seu conhecimento"}, 
        contexto anterior e pergunta do usu√°rio, forne√ßa a melhor resposta poss√≠vel.
        
        Contexto anterior:
        {context}
        
        Diretrizes:
        - Seja completo mas conciso
        - Cite fontes quando usar dados de pesquisa
        - Mantenha foco t√©cnico
        - Inclua exemplos pr√°ticos quando relevante
        
        Pergunta: {question}
        An√°lise: {analysis}
        {research_context}
        """
        
        resposta = op.ChatCompletion.create(
            model="gpt-4",
            messages=[{"role": "user", "content": synthesis_prompt}],
            max_tokens=1800,
            temperature=0.6
        ).choices[0].message['content'].strip()
        
        # Etapa 4: Auto-avalia√ß√£o
        self.update_thought("Auto-avaliando resposta...", 10)
        evaluation_prompt = f"""
        Avalie a qualidade da resposta gerada considerando:
        - Precis√£o t√©cnica
        - Relev√¢ncia para a pergunta
        - Clareza e organiza√ß√£o
        - Utilidade pr√°tica
        - Atualiza√ß√£o das informa√ß√µes
        
        Resposta: {resposta}
        
        Forne√ßa:
        1. Pontua√ß√£o (0-10)
        2. Tr√™s melhorias potenciais
        3. Decis√£o: MANTER ou REFINAR
        """
        
        evaluation = op.ChatCompletion.create(
            model="gpt-4",
            messages=[{"role": "user", "content": evaluation_prompt}],
            max_tokens=300,
            temperature=0.4
        ).choices[0].message['content'].strip()
        
        if "REFINAR" in evaluation:
            self.update_thought("Refinando resposta...", 10)
            refinement_prompt = f"""
            Com base na avalia√ß√£o, refine a resposta para melhor atendimento ao usu√°rio.
            
            Avalia√ß√£o: {evaluation}
            Resposta original: {resposta}
            
            Forne√ßa apenas a resposta refinada, sem coment√°rios.
            """
            
            resposta = op.ChatCompletion.create(
                model="gpt-4",
                messages=[{"role": "user", "content": refinement_prompt}],
                max_tokens=1800,
                temperature=0.5
            ).choices[0].message['content'].strip()
        
        # Armazenar contexto
        self.remember_context(
            conversation=f"P: {question}\nR: {resposta}",
            important_facts={
                "tema_principal": question[:50] + "...",
                "decisoes": evaluation
            }
        )
        
        self.update_thought("Processamento completo!", 10)
        return resposta, analysis, research_results, evaluation
    
    def get_current_knowledge(self):
        expertise = ", ".join([f"{k}({v}/10)" for k, v in self.knowledge['expertise'].items()])
        behavior = ", ".join([f"{k}: {v}" for k, v in self.knowledge['behavior'].items()])
        return f"Expertise: {expertise}\nComportamento: {behavior}"
    
    def auto_optimize_parameters(self):
        """Otimiza automaticamente os par√¢metros de opera√ß√£o"""
        if len(self.performance_metrics['response_times']) < 5:
            return
        
        self.update_thought("Otimizando par√¢metros...", 10)
        # Modelo preditivo para otimiza√ß√£o
        X = np.array(self.performance_metrics['response_times']).reshape(-1, 1)
        y = np.array(self.performance_metrics['user_ratings'])
        
        if len(y) > 1:
            try:
                model = LinearRegression()
                model.fit(X, y)
                optimal_time = np.argmax(model.predict(X))
                
                # Ajustar par√¢metros de opera√ß√£o
                self.knowledge['behavior']['response_speed'] = max(0.5, min(2.0, optimal_time/10))
                self.log_update(f"Par√¢metros otimizados: velocidade={self.knowledge['behavior']['response_speed']:.1f}")
                self.knowledge['expertise']['auto_optimization'] = min(10, self.knowledge['expertise'].get('auto_optimization', 5) + 1)
            except:
                pass
        self.update_thought("Otimiza√ß√£o conclu√≠da", 10)
    
    def execute_scheduled_tasks(self):
        """Executa tarefas agendadas automaticamente"""
        now = datetime.now()
        tasks_to_remove = []
        
        for i, task in enumerate(self.scheduled_tasks):
            task_time = datetime.strptime(task['next_run'], "%Y-%m-%d %H:%M")
            if now >= task_time:
                try:
                    self.update_thought(f"Executando tarefa: {task['name']}", 5)
                    
                    # Executar pesquisa autom√°tica
                    if task['type'] == 'research':
                        results = self.web_search(task['query'])
                        self.knowledge['expertise'][task['topic']] = min(10, self.knowledge['expertise'].get(task['topic'], 5) + 1)
                        self.log_update(f"Pesquisa agendada: {task['query']} - Conhecimento em {task['topic']} aumentado")
                    
                    # Reprogramar a tarefa
                    if task['frequency'] == 'daily':
                        task['next_run'] = (now + timedelta(days=1)).strftime("%Y-%m-%d %H:%M")
                    elif task['frequency'] == 'weekly':
                        task['next_run'] = (now + timedelta(weeks=1)).strftime("%Y-%m-%d %H:%M")
                    elif task['frequency'] == 'once':
                        tasks_to_remove.append(i)
                    
                except Exception as e:
                    self.log_update(f"Erro na tarefa {task['name']}: {str(e)}")
        
        # Remover tarefas √∫nicas que foram executadas
        for i in sorted(tasks_to_remove, reverse=True):
            if i < len(self.scheduled_tasks):
                del self.scheduled_tasks[i]
        
        self.save_tasks()
        self.update_thought("Tarefas agendadas processadas", 10)
    
    def auto_generate_tasks(self):
        """Gera automaticamente tarefas baseadas em gaps de conhecimento"""
        self.update_thought("Identificando gaps de conhecimento...", 10)
        topics = ["python", "ai", "web development", "data science", "machine learning"]
        knowledge_gaps = []
        
        for topic in topics:
            if self.knowledge['expertise'].get(topic, 0) < 7:
                knowledge_gaps.append(topic)
        
        for topic in knowledge_gaps:
            new_task = {
                "name": f"Atualizar {topic}",
                "type": "research",
                "query": f"avan√ßos recentes em {topic}",
                "topic": topic,
                "frequency": "weekly",
                "next_run": (datetime.now() + timedelta(days=1)).strftime("%Y-%m-%d 09:00")
            }
            
            if new_task not in self.scheduled_tasks:
                self.scheduled_tasks.append(new_task)
        
        self.save_tasks()
        self.log_update(f"Tarefas geradas para {len(knowledge_gaps)} gaps de conhecimento")
        self.update_thought(f"Geradas {len(knowledge_gaps)} novas tarefas", 10)
    
    def api_action_executor(self, action_command):
        """Executa a√ß√µes atrav√©s de APIs externas"""
        try:
            self.update_thought(f"Processando comando: {action_command[:30]}...", 10)
            # An√°lise do comando
            parse_prompt = f"""
            Extraia informa√ß√µes de: {action_command}
            Formato resposta JSON:
            {{
                "action_type": "schedule|data_fetch|other",
                "title": "t√≠tulo se aplic√°vel",
                "date": "YYYY-MM-DD",
                "time": "HH:MM",
                "duration": minutos,
                "query": "consulta se aplic√°vel"
            }}
            """
            
            parsed = op.ChatCompletion.create(
                model="gpt-4",
                messages=[{"role": "user", "content": parse_prompt}],
                max_tokens=200,
                temperature=0.1
            ).choices[0].message['content'].strip()
            
            # Limpar e converter para JSON
            parsed = parsed[parsed.find('{'):parsed.rfind('}')+1]
            data = json.loads(parsed)
            
            # Executar a√ß√£o baseada no tipo
            if data['action_type'] == 'schedule':
                st.success(f"Reuni√£o '{data.get('title', '')}' agendada para {data.get('date', '')} √†s {data.get('time', '')} por {data.get('duration', 30)} minutos")
                self.update_thought("Reuni√£o agendada com sucesso", 10)
                return True
            elif data['action_type'] == 'data_fetch':
                st.success(f"Dados recuperados para: {data.get('query', '')}")
                self.update_thought("Dados recuperados com sucesso", 10)
                return True
            else:
                st.info(f"A√ß√£o executada: {action_command}")
                self.update_thought("A√ß√£o executada com sucesso", 10)
                return True
            
        except Exception as e:
            self.update_thought(f"Erro na execu√ß√£o: {str(e)}", 0)
            st.error(f"Erro na execu√ß√£o de a√ß√£o: {str(e)}")
            return False
    
    def monitor_performance(self, response_time, accuracy, user_rating):
        """Monitora e registra m√©tricas de performance"""
        self.performance_metrics['response_times'].append(response_time)
        self.performance_metrics['accuracy_scores'].append(accuracy)
        self.performance_metrics['user_ratings'].append(user_rating)
        
        # Auto-otimiza√ß√£o peri√≥dica
        if len(self.performance_metrics['response_times']) % 5 == 0:
            self.auto_optimize_parameters()
            
        # Gera√ß√£o autom√°tica de tarefas
        if len(self.performance_metrics['response_times']) % 7 == 0:
            self.auto_generate_tasks()
            
        # Aprendizado cont√≠nuo
        if len(self.performance_metrics['response_times']) % 10 == 0:
            self.continuous_learning()

# Inicializar sistema aut√¥nomo
if 'assistant' not in st.session_state:
    st.session_state.assistant = AutonomousAssistant()

# Hist√≥rico de conversa
if 'history' not in st.session_state:
    st.session_state['history'] = []

# Vari√°veis de configura√ß√£o na sess√£o
if 'mostrar_pensamento' not in st.session_state:
    st.session_state.mostrar_pensamento = True
    
if 'idioma' not in st.session_state:
    st.session_state.idioma = 'pt'
    
if 'velocidade' not in st.session_state:
    st.session_state.velocidade = 1.0

# Fun√ß√£o para humanizar texto
def humanizar_texto(texto):
    """Adiciona pausas e entona√ß√£o natural ao texto"""
    # Adiciona pausas ap√≥s pontua√ß√µes
    texto = re.sub(r'([.!?;])', r'\1 ', texto)
    
    # Quebras de linha para pausas naturais
    texto = re.sub(r'\.\s', '.\n\n', texto)
    texto = re.sub(r',\s', ',\n', texto)
    
    # Simplifica√ß√£o de express√µes complexas
    substituicoes = {
        r'\b(\d+)h(\d+)?\b': lambda m: f"{m.group(1)} horas" if not m.group(2) else f"{m.group(1)} horas e {m.group(2)} minutos",
        r'\bR\$\s*(\d+[\d,.]*)\b': lambda m: f"{m.group(1).replace('.', '')} reais",
        r'\b(\d+)m\b': lambda m: f"{m.group(1)} metros",
        r'\b(\d+)kg\b': lambda m: f"{m.group(1)} quilos"
    }
    
    for padrao, repl in substituicoes.items():
        texto = re.sub(padrao, repl, texto)
    
    # Varia√ß√µes de √™nfase
    palavras = texto.split()
    for i in range(0, len(palavras), 5):
        if random.random() > 0.7 and len(palavras[i]) > 3:
            palavras[i] = f"*{palavras[i]}*"
    
    return ' '.join(palavras)

# Fun√ß√µes de √°udio
def gravar_audio(duracao=5, arquivo="audio.wav"):
    try:
        audio = pyaudio.PyAudio()
        stream = audio.open(
            format=pyaudio.paInt16,
            channels=1,
            rate=16000,
            input=True,
            frames_per_buffer=1024
        )
        
        frames = []
        
        for i in range(0, int(16000 / 1024 * duracao)):
            data = stream.read(1024, exception_on_overflow=False)
            frames.append(data)
        
        stream.stop_stream()
        stream.close()
        audio.terminate()
        
        with wave.open(arquivo, 'wb') as wf:
            wf.setnchannels(1)
            wf.setsampwidth(audio.get_sample_size(pyaudio.paInt16))
            wf.setframerate(16000)
            wf.writeframes(b''.join(frames))
            
        return arquivo
    except Exception as e:
        st.error(f"Erro na grava√ß√£o: {str(e)}")
        return None

def transcrever_audio(arquivo):
    try:
        if not arquivo or not os.path.exists(arquivo):
            return ""
            
        with open(arquivo, 'rb') as f:
            resultado = op.Audio.transcribe('whisper-1', f, language='pt')
            return resultado['text'].strip()
    except Exception as e:
        st.error(f"Erro na transcri√ß√£o: {str(e)}")
        return ""

def falar_texto(texto, lang='pt'):
    try:
        if not texto:
            return None
            
        # Pr√©-processamento para humanizar a fala
        texto_humanizado = humanizar_texto(texto)
        
        # Ajustar velocidade baseado na configura√ß√£o
        velocidade = st.session_state.velocidade
        slow = velocidade < 1.0
        
        # Par√¢metros ajustados para naturalidade
        tts = gTTS(
            text=texto_humanizado, 
            lang=lang,
            tld='com.br',  # Sotaque brasileiro
            slow=slow,
            lang_check=False
        )
        
        # Gera√ß√£o de √°udio em mem√≥ria
        audio_buffer = io.BytesIO()
        tts.write_to_fp(audio_buffer)
        audio_bytes = audio_buffer.getvalue()
        
        # Reproduzir √°udio
        st.audio(audio_bytes, format='audio/mp3', autoplay=True)
        return audio_bytes
    except Exception as e:
        st.error(f"Erro na s√≠ntese de voz: {str(e)}")
        return None

# ============= SISTEMA DE TAREFAS AUTOM√ÅTICAS =============
def task_scheduler_thread():
    """Thread para execu√ß√£o de tarefas agendadas"""
    while True:
        try:
            if 'assistant' in st.session_state:
                st.session_state.assistant.execute_scheduled_tasks()
            time.sleep(60)
        except:
            pass

# Iniciar thread em segundo plano
if 'scheduler_started' not in st.session_state:
    st.session_state.scheduler_started = True
    Thread(target=task_scheduler_thread, daemon=True).start()

# Configura√ß√£o da p√°gina
st.set_page_config(page_title="Assistente Cognitivo Py", layout="wide", page_icon="ü§ñ")

# CSS personalizado com anima√ß√µes de pensamento e responsividade
st.markdown("""
<style>
    /* Estilo geral */
    .stApp { 
        background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
        color: #ffffff;
        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    }
    
    /* Barra lateral */
    [data-testid="stSidebar"] {
        background: linear-gradient(180deg, #0f3460 0%, #1a1a2e 100%) !important;
        border-right: 1px solid #4e54c8;
        box-shadow: 5px 0 15px rgba(0, 0, 0, 0.4);
    }
    
    /* Bot√µes da barra lateral */
    .stButton>button {
        background: linear-gradient(45deg, #4e54c8, #8f94fb) !important;
        color: white !important;
        border-radius: 8px !important;
        border: none !important;
        padding: 12px 24px !important;
        font-weight: 600 !important;
        width: 100%;
        margin: 8px 0;
        transition: all 0.3s ease !important;
    }
    .stButton>button:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 15px rgba(0, 0, 0, 0.3);
    }
    
    /* Bot√£o de grava√ß√£o */
    .btn-record {
        font-size: 2rem !important;
        width: 120px !important;
        height: 120px !important;
        border-radius: 50% !important;
        background: linear-gradient(135deg, #ff416c, #ff4b2b) !important;
        margin: 20px auto !important;
        display: block !important;
        box-shadow: 0 4px 20px rgba(255, 75, 43, 0.4);
    }
    
    /* Abas */
    [data-baseweb="tab-list"] {
        gap: 10px;
    }
    [data-baseweb="tab"] {
        padding: 12px 24px !important;
        background: #1a1a2e !important;
        border-radius: 8px !important;
        margin: 0 5px !important;
        transition: all 0.3s !important;
    }
    [data-baseweb="tab"]:hover {
        background: #4e54c8 !important;
    }
    [aria-selected="true"] {
        background: linear-gradient(45deg, #4e54c8, #8f94fb) !important;
        font-weight: bold !important;
    }
    
    /* Inputs */
    .stTextInput>div>div>input {
        background: rgba(255, 255, 255, 0.1) !important;
        color: white !important;
        border-radius: 8px !important;
        border: 1px solid #4e54c8 !important;
        padding: 12px !important;
    }
    
    /* Hist√≥rico */
    .history-item {
        background: rgba(26, 26, 46, 0.7);
        border-radius: 10px;
        padding: 15px;
        margin: 10px 0;
        border-left: 4px solid #4e54c8;
    }
    
    /* Pensamento */
    .thinking-bubble {
        background: #2c3e50;
        border-radius: 20px;
        padding: 15px;
        margin: 15px 0;
        border: 1px solid #3498db;
        animation: pulse 2s infinite;
    }
    
    @keyframes pulse {
        0% { box-shadow: 0 0 0 0 rgba(52, 152, 219, 0.7); }
        70% { box-shadow: 0 0 0 10px rgba(52, 152, 219, 0); }
        100% { box-shadow: 0 0 0 0 rgba(52, 152, 219, 0); }
    }
    
    /* Estilos para autonomia */
    .autonomous-decision {
        background: linear-gradient(135deg, #1e3c72, #2a5298);
        border-radius: 15px;
        padding: 20px;
        margin: 15px 0;
        border-left: 5px solid #4e54c8;
        box-shadow: 0 5px 15px rgba(0, 0, 0, 0.3);
    }
    
    .research-card {
        background: rgba(30, 30, 50, 0.9);
        border-radius: 10px;
        padding: 15px;
        margin: 10px 0;
        border: 1px solid #4e54c8;
    }
    
    .evaluation-panel {
        background: rgba(40, 40, 60, 0.9);
        border-radius: 10px;
        padding: 15px;
        margin: 15px 0;
        border: 2px solid #ff4b5c;
    }
    
    /* Estilos de voz */
    .voice-panel {
        background: linear-gradient(135deg, #4a00e0, #8e2de2);
        border-radius: 15px;
        padding: 20px;
        margin: 15px 0;
        text-align: center;
    }
    
    .voice-visualizer {
        display: flex;
        justify-content: center;
        height: 100px;
        align-items: flex-end;
        margin: 20px 0;
    }
    
    .voice-bar {
        width: 10px;
        background: #00d2ff;
        margin: 0 2px;
        border-radius: 5px 5px 0 0;
        animation: voicePulse 1.5s infinite;
    }
    
    @keyframes voicePulse {
        0% { height: 10%; }
        50% { height: 90%; }
        100% { height: 10%; }
    }
    
    /* Automa√ß√£o */
    .automation-card {
        background: rgba(40, 40, 60, 0.8);
        border-radius: 10px;
        padding: 15px;
        margin: 10px 0;
        border: 1px solid #8f94fb;
    }
    
    /* Anima√ß√µes de pensamento */
    @keyframes thoughtPulse {
        0% { transform: scale(1); opacity: 0.7; }
        50% { transform: scale(1.02); opacity: 1; }
        100% { transform: scale(1); opacity: 0.7; }
    }
    
    .thought-container {
        background: rgba(40, 40, 60, 0.9);
        border-radius: 15px;
        padding: 20px;
        margin: 20px 0;
        border-left: 5px solid #4e54c8;
        animation: thoughtPulse 3s infinite;
        box-shadow: 0 4px 20px rgba(0, 0, 0, 0.3);
    }
    
    .thought-bubble {
        background: rgba(50, 50, 80, 0.95);
        border-radius: 20px;
        padding: 15px;
        margin: 10px 0;
        border: 1px solid #3498db;
    }
    
    .progress-container {
        background: rgba(30, 30, 50, 0.7);
        border-radius: 10px;
        padding: 10px;
        margin: 15px 0;
    }
    
    /* Estilos de colabora√ß√£o */
    .collaboration-panel {
        background: linear-gradient(135deg, #2a0c4e, #4a1b8c);
        border-radius: 15px;
        padding: 20px;
        margin: 15px 0;
    }
    
    .collaboration-message {
        background: rgba(50, 50, 80, 0.8);
        border-radius: 10px;
        padding: 12px;
        margin: 8px 0;
        border-left: 3px solid #8f94fb;
    }
    
    /* Responsividade */
    @media (max-width: 768px) {
        .stApp > div {
            padding: 8px !important;
        }
        [data-baseweb="tab"] {
            padding: 8px 12px !important;
            font-size: 0.9rem !important;
        }
        .stTextArea textarea {
            height: 120px !important;
        }
        .stButton>button {
            padding: 10px 18px !important;
        }
        .btn-record {
            width: 90px !important;
            height: 90px !important;
            font-size: 1.5rem !important;
        }
        .colab-user {
            font-size: 0.9rem;
        }
    }
</style>
""", unsafe_allow_html=True)

# Barra lateral
with st.sidebar:
    st.title("ü§ñ Assistente Cognitivo")
    st.subheader("Sistema de Tomada de Decis√£o")
    
    # Painel de conhecimento
    st.markdown("### üß† Conhecimento Atual")
    for area, nivel in st.session_state.assistant.knowledge['expertise'].items():
        st.progress(nivel/10, text=f"{area.capitalize()} ({nivel}/10)")
    
    # Configura√ß√µes
    st.markdown("### ‚öôÔ∏è Configura√ß√µes")
    st.session_state.mostrar_pensamento = st.checkbox("Mostrar processo de decis√£o", value=st.session_state.mostrar_pensamento)
    
    st.markdown("### üéö Par√¢metros de Voz")
    st.session_state.velocidade = st.slider("Velocidade", 0.5, 2.0, 1.0, 0.1)
    st.session_state.idioma = st.selectbox("Idioma", ["pt", "en"], index=0 if st.session_state.idioma == 'pt' else 1)
    
    # Gerenciamento
    if st.button("üßπ Limpar Hist√≥rico", key="clear_history_btn"):
        st.session_state['history'] = []
        st.success("Hist√≥rico limpo!")
    
    if st.button("üîÑ Atualizar Conhecimento", key="update_knowledge_btn"):
        st.session_state.assistant.log_update("Atualiza√ß√£o manual de conhecimento")
        st.success("Conhecimento atualizado!")
    
    # Controle de voz
    st.markdown("### üé§ Controles de Voz")
    duracao_gravacao = st.slider("Dura√ß√£o da grava√ß√£o (seg)", 3, 10, 5, key="voice_duration")
    
    # Colabora√ß√£o
    st.markdown("### üë• Colabora√ß√£o")
    new_user = st.text_input("Adicionar colaborador:", key="new_collaborator")
    if st.button("‚ûï Adicionar", key="add_collaborator_btn") and new_user:
        if st.session_state.assistant.add_collaborator(new_user):
            st.success(f"{new_user} adicionado √† sess√£o!")
        else:
            st.warning("Usu√°rio j√° est√° na sess√£o")

# Abas principais
tab_chat, tab_auto, tab_research, tab_voice, tab_automation, tab_docs, tab_colab = st.tabs(
    ["üí¨ Chat", "üß† Autonomia", "üåê Pesquisas", "üé§ Voz", "‚öôÔ∏è Automa√ß√£o", "üìÑ Documentos", "üë• Colabora√ß√£o"]
)

# Inicializar elementos de UI para pensamento
if 'thought_ui' not in st.session_state:
    st.session_state.thought_ui = st.empty()
if 'progress_ui' not in st.session_state:
    st.session_state.progress_ui = st.empty()

# Atualizar pensamento inicial
st.session_state.thought_ui.markdown(f"üí≠ **Pensando:** {st.session_state.assistant.current_thought}")
st.session_state.progress_ui.progress(st.session_state.assistant.current_progress)

# Aba Chat
with tab_chat:
    st.header("üß† Assistente Cognitivo")
    
    # Painel de pensamento
    with st.container():
        st.markdown("### Estado Cognitivo Atual")
        col1, col2 = st.columns([3, 1])
        with col1:
            st.session_state.thought_ui = st.empty()
        with col2:
            st.session_state.progress_ui = st.progress(st.session_state.assistant.current_progress)
    
    pergunta = st.text_area("Digite sua pergunta:", height=150, key="chat_input",
                           placeholder="Pergunte sobre Python, IA, ou pe√ßa para pesquisar na web...")
    
    if st.button("üöÄ Enviar Pergunta", use_container_width=True, key="send_question_btn") and pergunta:
        # Reiniciar indicadores
        st.session_state.assistant.current_thought = "Processando sua pergunta..."
        st.session_state.assistant.current_progress = 0
        st.session_state.thought_ui.markdown(f"üí≠ **Pensando:** {st.session_state.assistant.current_thought}")
        st.session_state.progress_ui.progress(0)
        
        start_time = time.time()
        with st.spinner("Processando de forma aut√¥noma..."):
            resposta, analysis, research, evaluation = st.session_state.assistant.analyze_and_decide(pergunta)
            end_time = time.time()
            
            # Coletar m√©tricas de precis√£o
            accuracy_prompt = f"""
            Avalie a precis√£o da resposta em uma escala de 0-1:
            Pergunta: {pergunta}
            Resposta: {resposta}
            """
            try:
                accuracy = op.ChatCompletion.create(
                    model="gpt-3.5-turbo",
                    messages=[{"role": "user", "content": accuracy_prompt}],
                    max_tokens=10,
                    temperature=0.0
                ).choices[0].message['content'].strip()
                accuracy = float(accuracy)
            except:
                accuracy = 0.8
            
            # Registrar m√©tricas
            st.session_state.assistant.monitor_performance(
                response_time=end_time - start_time,
                accuracy=accuracy,
                user_rating=5
            )
            
            # Registro no hist√≥rico
            st.session_state['history'].append({
                "pergunta": pergunta,
                "resposta": resposta,
                "analise": analysis,
                "pesquisa": research,
                "avaliacao": evaluation
            })
        
        # Exibi√ß√£o do processo de decis√£o
        if st.session_state.mostrar_pensamento:
            with st.expander("üìä Detalhes do Processamento"):
                st.markdown(f"**An√°lise da pergunta:**\n{analysis}")
                
                if research:
                    st.markdown("**üîç Pesquisa realizada (Google API):**")
                    for i, res in enumerate(research):
                        st.markdown(f"{i+1}. **[{res['title']}]({res['url']})**\n{res['snippet']}")
                
                st.markdown(f"**üìä Auto-avalia√ß√£o:**\n{evaluation}")
                
                # Mostrar contexto utilizado
                st.markdown("**üß† Contexto utilizado:**")
                st.code(st.session_state.assistant.recall_context())
        
        # Exibi√ß√£o da resposta
        st.markdown(f"""
        <div class='history-item'>
            <h4>Voc√™:</h4>
            <p>{pergunta}</p>
            <h4>Assistente:</h4>
            <p>{resposta}</p>
        </div>
        """, unsafe_allow_html=True)
        
        # Reproduzir resposta em √°udio
        falar_texto(resposta, lang=st.session_state.idioma)

# Aba Autonomia
with tab_auto:
    st.header("üß† Sistema de Tomada de Decis√£o Aut√¥noma")
    
    st.markdown("""
    <div class='autonomous-decision'>
        <h4>Arquitetura Cognitiva</h4>
        <p>Este assistente toma decis√µes independentes atrav√©s de 4 est√°gios:</p>
        <ol>
            <li><strong>An√°lise da Pergunta</strong>: Compreens√£o profunda da necessidade do usu√°rio</li>
            <li><strong>Decis√£o de Pesquisa</strong>: Avalia√ß√£o aut√¥noma da necessidade de dados externos</li>
            <li><strong>S√≠ntese de Resposta</strong>: Combina√ß√£o de conhecimento interno e pesquisa</li>
            <li><strong>Auto-avalia√ß√£o</strong>: Cr√≠tica e refinamento da resposta antes de entregar</li>
        </ol>
    </div>
    """, unsafe_allow_html=True)
    
    # Exibir √∫ltima an√°lise
    if st.session_state['history']:
        ultimo = st.session_state['history'][-1]
        st.subheader("√öltimo Processo Decis√≥rio")
        
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("**‚ùì Pergunta Original**")
            st.info(ultimo['pergunta'])
            
            st.markdown("**üß† An√°lise Estrat√©gica**")
            st.code(ultimo['analise'], language='text')
        
        with col2:
            st.markdown("**üìù Resposta Gerada**")
            st.success(ultimo['resposta'])
            
            st.markdown("**üìä Auto-avalia√ß√£o**")
            st.warning(ultimo['avaliacao'])
    else:
        st.info("Nenhum hist√≥rico de decis√£o dispon√≠vel. Fa√ßa uma pergunta na aba Chat.")

# Aba Pesquisas
with tab_research:
    st.header("üåê Hist√≥rico de Pesquisas Aut√¥nomas")
    
    if st.session_state.assistant.search_history:
        for search in reversed(st.session_state.assistant.search_history):
            with st.expander(f"üîç {search['query']} ({search['source']})"):
                st.caption(f"Pesquisado em {search['timestamp'][:19].replace('T', ' ')}")
                
                for i, res in enumerate(search['results']):
                    st.markdown(f"""
                    <div class='research-card'>
                        <h4>{i+1}. {res['title']}</h4>
                        <p>{res['snippet']}</p>
                        <a href="{res['url']}" target="_blank">Ver fonte</a>
                    </div>
                    """, unsafe_allow_html=True)
    else:
        st.info("O assistente ainda n√£o realizou pesquisas. Fa√ßa perguntas que requeiram dados atualizados.")

# Aba Voz
with tab_voice:
    st.header("üé§ Modo Voz Completo")
    
    st.markdown("""
    <div class='voice-panel'>
        <h3>Intera√ß√£o por Voz</h3>
        <p>Converse naturalmente com o assistente usando seu microfone</p>
        
        <div class='voice-visualizer'>
            <div class='voice-bar'></div>
            <div class='voice-bar'></div>
            <div class='voice-bar'></div>
            <div class='voice-bar'></div>
            <div class='voice-bar'></div>
            <div class='voice-bar'></div>
            <div class='voice-bar'></div>
            <div class='voice-bar'></div>
            <div class='voice-bar'></div>
            <div class='voice-bar'></div>
        </div>
    </div>
    """, unsafe_allow_html=True)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("Grava√ß√£o de Voz")
        if st.button("üé§ Iniciar Grava√ß√£o", use_container_width=True, key="voice_record_btn"):
            arquivo = "voice_temp.wav"
            resultado = gravar_audio(duracao_gravacao, arquivo)
            
            if resultado:
                pergunta = transcrever_audio(arquivo)
                os.remove(arquivo)
                
                if pergunta:
                    st.session_state.voice_question = pergunta
                    st.success("√Åudio transcrito com sucesso!")
                    st.markdown(f"**Pergunta detectada:** {pergunta}")
                    
                    # An√°lise de sentimento
                    sentiment = st.session_state.assistant.analyze_sentiment(arquivo)
                    st.info(f"Sentimento detectado: {sentiment.capitalize()}")
                else:
                    st.error("N√£o foi poss√≠vel transcrever o √°udio")
    
    with col2:
        st.subheader("Processamento")
        if 'voice_question' in st.session_state and st.session_state.voice_question:
            if st.button("üß† Processar Pergunta", use_container_width=True, key="process_voice_btn"):
                # Reiniciar indicadores
                st.session_state.assistant.current_thought = "Processando pergunta de voz..."
                st.session_state.assistant.current_progress = 0
                st.session_state.thought_ui.markdown(f"üí≠ **Pensando:** {st.session_state.assistant.current_thought}")
                st.session_state.progress_ui.progress(0)
                
                start_time = time.time()
                with st.spinner("Processando pergunta de voz..."):
                    resposta, analysis, research, evaluation = st.session_state.assistant.analyze_and_decide(st.session_state.voice_question)
                    end_time = time.time()
                    
                    # Coletar m√©tricas de precis√£o
                    accuracy_prompt = f"""
                    Avalie a precis√£o da resposta em uma escala de 0-1:
                    Pergunta: {st.session_state.voice_question}
                    Resposta: {resposta}
                    """
                    try:
                        accuracy = op.ChatCompletion.create(
                            model="gpt-3.5-turbo",
                            messages=[{"role": "user", "content": accuracy_prompt}],
                            max_tokens=10,
                            temperature=0.0
                        ).choices[0].message['content'].strip()
                        accuracy = float(accuracy)
                    except:
                        accuracy = 0.8
                    
                    # Registrar m√©tricas
                    st.session_state.assistant.monitor_performance(
                        response_time=end_time - start_time,
                        accuracy=accuracy,
                        user_rating=5
                    )
                    
                    # Registro no hist√≥rico
                    st.session_state['history'].append({
                        "pergunta": st.session_state.voice_question,
                        "resposta": resposta,
                        "analise": analysis,
                        "pesquisa": research,
                        "avaliacao": evaluation,
                        "modo": "voz"
                    })
                
                # Exibir resposta
                st.markdown(f"""
                <div class='history-item'>
                    <h4>Resposta:</h4>
                    <p>{resposta}</p>
                </div>
                """, unsafe_allow_html=True)
                
                # Reproduzir √°udio
                falar_texto(resposta, lang=st.session_state.idioma)
        
    # Hist√≥rico de voz
    st.subheader("Hist√≥rico de Conversas por Voz")
    voice_history = [h for h in st.session_state['history'] if h.get('modo') == 'voz']
    
    if voice_history:
        for i, item in enumerate(reversed(voice_history)):
            with st.expander(f"Conversa de voz {len(voice_history)-i}"):
                st.markdown(f"**Voc√™:** {item['pergunta']}")
                st.markdown(f"**Assistente:** {item['resposta']}")
                # Regenerar √°udio para reprodu√ß√£o
                audio_bytes = falar_texto(item['resposta'], lang=st.session_state.idioma)
                if audio_bytes:
                    st.audio(audio_bytes, format='audio/mp3')
    else:
        st.info("Nenhuma conversa por voz registrada. Grave sua primeira pergunta!")

# Aba Automa√ß√£o
with tab_automation:
    st.header("‚öôÔ∏è Sistema de Automa√ß√£o Cognitiva")
    
    # Painel de pensamento
    with st.container():
        st.markdown("### Estado Cognitivo Atual")
        col1, col2 = st.columns([3, 1])
        with col1:
            st.session_state.thought_ui.markdown(f"üí≠ **Pensando:** {st.session_state.assistant.current_thought}")
        with col2:
            st.session_state.progress_ui.progress(st.session_state.assistant.current_progress)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üìä Performance do Sistema")
        if st.session_state.assistant.performance_metrics['response_times']:
            metrics_df = pd.DataFrame({
                "Tempo Resposta": st.session_state.assistant.performance_metrics['response_times'],
                "Acur√°cia": st.session_state.assistant.performance_metrics['accuracy_scores'],
                "Avalia√ß√£o": st.session_state.assistant.performance_metrics['user_ratings']
            })
            st.line_chart(metrics_df)
            
            avg_time = np.mean(st.session_state.assistant.performance_metrics['response_times'])
            avg_accuracy = np.mean(st.session_state.assistant.performance_metrics['accuracy_scores'])
            avg_rating = np.mean(st.session_state.assistant.performance_metrics['user_ratings'])
            
            st.metric("‚è± Tempo M√©dio", f"{avg_time:.1f}s")
            st.metric("üéØ Acur√°cia M√©dia", f"{avg_accuracy*100:.1f}%")
            st.metric("‚≠ê Avalia√ß√£o M√©dia", f"{avg_rating:.1f}/5")
        else:
            st.info("Coletando dados de performance...")
    
    with col2:
        st.subheader("ü§ñ Auto-Otimiza√ß√£o")
        nivel_automacao = st.session_state.assistant.knowledge['expertise'].get('auto_optimization', 0)
        st.progress(nivel_automacao/10, 
                   text=f"N√≠vel de Automa√ß√£o: {nivel_automacao}/10")
        
        if st.button("üîç Executar Diagn√≥stico", help="Analisa e otimiza par√¢metros internos", key="run_diagnostic_btn"):
            with st.spinner("Otimizando sistema..."):
                st.session_state.assistant.auto_optimize_parameters()
                st.success("Par√¢metros otimizados!")
        
        st.markdown("**Par√¢metros Atuais:**")
        st.json({
            "response_speed": st.session_state.assistant.knowledge['behavior'].get('response_speed', 1),
            "research_depth": st.session_state.assistant.knowledge['behavior'].get('research_depth', 2),
            "autonomy_level": st.session_state.assistant.knowledge['behavior'].get('autonomy_level', 3)
        })
    
    st.subheader("üìÖ Tarefas Agendadas")
    if st.button("üîÑ Gerar Tarefas Automaticamente", key="auto_tasks_gen_btn"):
        st.session_state.assistant.auto_generate_tasks()
        st.success("Tarefas geradas com base em gaps de conhecimento!")
    
    for i, task in enumerate(st.session_state.assistant.scheduled_tasks):
        with st.expander(f"‚è∞ {task['name']} - Pr√≥xima execu√ß√£o: {task['next_run']}"):
            st.markdown(f"**Tipo:** {task['type'].capitalize()}")
            st.markdown(f"**Frequ√™ncia:** {task['frequency']}")
            st.markdown(f"**T√≥pico:** {task['topic']}")
            
            if task['type'] == 'research':
                st.markdown(f"**Query:** `{task['query']}`")
            
            if st.button("Executar Agora", key=f"run_task_{i}"):
                st.session_state.assistant.execute_scheduled_tasks()
                st.experimental_rerun()
            
            if st.button("‚ùå Remover", key=f"del_task_{i}"):
                del st.session_state.assistant.scheduled_tasks[i]
                st.session_state.assistant.save_tasks()
                st.experimental_rerun()
    
    st.subheader("ü§ñ A√ß√µes por API")
    action_command = st.text_input("Comando de a√ß√£o:", placeholder="Ex: Agendar reuni√£o sobre IA amanh√£ √†s 10:00 por 1 hora", key="api_command")
    if st.button("Executar A√ß√£o", key="run_api_action_btn") and action_command:
        result = st.session_state.assistant.api_action_executor(action_command)
        if result:
            st.success("A√ß√£o executada com sucesso!")
        else:
            st.error("Falha na execu√ß√£o da a√ß√£o")
    
    # Plugins
    st.subheader("üß© Plugins Dispon√≠veis")
    for plugin, info in st.session_state.assistant.plugins.items():
        with st.expander(f"üîå {plugin.capitalize()} - {info['description']}"):
            if plugin == "stock":
                symbol = st.text_input("S√≠mbolo da a√ß√£o:", value="AAPL", key=f"stock_{plugin}")
                if st.button("Obter Cota√ß√£o", key=f"run_{plugin}"):
                    result = st.session_state.assistant.execute_plugin(plugin, {"symbol": symbol})
                    st.info(result)
            elif plugin == "weather":
                location = st.text_input("Localiza√ß√£o:", value="S√£o Paulo", key=f"weather_{plugin}")
                if st.button("Obter Previs√£o", key=f"run_{plugin}"):
                    result = st.session_state.assistant.execute_plugin(plugin, {"location": location})
                    st.info(result)
    
    # Execu√ß√£o em lote de tarefas
    st.subheader("üöÄ Execu√ß√£o em Lote")
    if st.button("‚ñ∂Ô∏è Executar Todas as Tarefas Agendadas", use_container_width=True, key="run_all_tasks_btn"):
        if st.session_state.assistant.scheduled_tasks:
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            for i, task in enumerate(st.session_state.assistant.scheduled_tasks):
                status_text.markdown(f"**Executando:** {task['name']}")
                progress_bar.progress(int((i+1)/len(st.session_state.assistant.scheduled_tasks)*100))
                
                # Atualizar pensamento
                st.session_state.assistant.update_thought(f"Processando tarefa: {task['name']}", 0)
                
                # Simular execu√ß√£o
                time.sleep(2)
                st.session_state.assistant.update_thought(f"Tarefa conclu√≠da: {task['name']}", 10)
                
                # Atualizar UI
                st.session_state.thought_ui.markdown(f"üí≠ **Pensando:** {st.session_state.assistant.current_thought}")
                st.session_state.progress_ui.progress(st.session_state.assistant.current_progress)
            
            status_text.success("Todas as tarefas foram executadas!")
            st.session_state.assistant.update_thought("Tarefas agendadas conclu√≠das", 10)
        else:
            st.warning("Nenhuma tarefa agendada para executar")

# Aba Documentos
with tab_docs:
    st.header("üìÑ An√°lise de Documentos")
    
    st.markdown("""
    <div class='autonomous-decision'>
        <h4>Processamento Inteligente de Documentos</h4>
        <p>Carregue documentos para an√°lise e extra√ß√£o autom√°tica de informa√ß√µes</p>
        <p>Formatos suportados: PDF, Word (DOCX), Texto (TXT)</p>
    </div>
    """, unsafe_allow_html=True)
    
    uploaded_file = st.file_uploader("Carregar documento", 
                                    type=["pdf", "docx", "txt"],
                                    key="document_uploader")
    
    if uploaded_file:
        st.success(f"Documento carregado: {uploaded_file.name} ({uploaded_file.size/1024:.1f} KB)")
        
        if st.button("üîç Analisar Documento", key="analyze_document_btn"):
            with st.spinner("Processando documento..."):
                summary, facts = st.session_state.assistant.process_document(uploaded_file)
                
                if summary:
                    st.subheader("Resumo do Documento")
                    st.write(summary)
                    
                    st.subheader("Fatos Extra√≠dos")
                    st.json(facts)
                    
                    # Armazenar fatos importantes
                    try:
                        facts_data = json.loads(facts)
                        if "pontos_chave" in facts_data:
                            st.session_state.assistant.remember_context(
                                conversation=f"Documento analisado: {uploaded_file.name}",
                                important_facts={
                                    "pontos_chave": ", ".join(facts_data["pontos_chave"][:3]),
                                    "recomendacoes": ", ".join(facts_data.get("recomendacoes", [])[:2])
                                }
                            )
                            st.success("Fatos importantes armazenados na mem√≥ria!")
                    except:
                        pass
                else:
                    st.error(f"Erro no processamento: {facts}")

# Aba Colabora√ß√£o
with tab_colab:
    st.header("üë• Modo Colaborativo")
    
    st.markdown("""
    <div class='collaboration-panel'>
        <h3>Trabalho em Equipe</h3>
        <p>Colabore com outros usu√°rios em tempo real</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Lista de colaboradores
    st.subheader("üë§ Participantes")
    if st.session_state.collaboration["users"]:
        for user in st.session_state.collaboration["users"]:
            st.markdown(f"- {user}")
    else:
        st.info("Nenhum participante adicionado. Use a barra lateral para adicionar colaboradores.")
    
    # Chat colaborativo
    st.subheader("üí¨ Conversa Colaborativa")
    col1, col2 = st.columns([3, 1])
    with col1:
        message = st.text_input("Digite sua mensagem:", key="collab_message")
    with col2:
        if st.button("Enviar", key="send_collab_msg") and message:
            st.session_state.assistant.add_collaboration_message("Voc√™", message)
           
    # Hist√≥rico de mensagens
    if st.session_state.collaboration["messages"]:
        for msg in st.session_state.collaboration["messages"][-10:]:
            st.markdown(f"""
            <div class='collaboration-message'>
                <strong>{msg['user']}</strong> ({msg['timestamp']}):
                <p>{msg['message']}</p>
            </div>
            """, unsafe_allow_html=True)
    else:
        st.info("Nenhuma mensagem na conversa colaborativa")
    
    # Compartilhamento de arquivos
    st.subheader("üìé Compartilhar Arquivos")
    colab_file = st.file_uploader("Selecionar arquivo para compartilhar", key="collab_file_upload")
    if st.button("Compartilhar Arquivo", key="share_file_btn") and colab_file:
        file_info = {
            "name": colab_file.name,
            "type": colab_file.type,
            "size": f"{colab_file.size/1024:.1f} KB"
        }
        st.session_state.assistant.share_file("Voc√™", file_info)
        st.success("Arquivo compartilhado com sucesso!")
        st.experimental_rerun()
    
    # Arquivos compartilhados
    st.subheader("üìÇ Arquivos Compartilhados")
    if st.session_state.collaboration["shared_files"]:
        for file in st.session_state.collaboration["shared_files"][-5:]:
            st.markdown(f"""
            <div class='collaboration-message'>
                <strong>{file['user']}</strong> ({file['timestamp']}):
                <p>{file['file']['name']} ({file['file']['type']}, {file['file']['size']})</p>
            </div>
            """, unsafe_allow_html=True)
    else:
        st.info("Nenhum arquivo compartilhado ainda")

# Business Intelligence Dashboard
st.sidebar.markdown("---")
if st.sidebar.button("üìä Business Intelligence", key="bi_dashboard_btn"):
    st.session_state.show_bi = True
    
if 'show_bi' in st.session_state and st.session_state.show_bi:
    with st.expander("üìà DASHBOARD DE BUSINESS INTELLIGENCE", expanded=True):
        report, fig = st.session_state.assistant.business_intelligence_dashboard()
        
        if isinstance(report, str):
            st.markdown(report)
        else:
            st.markdown(report, unsafe_allow_html=True)
        
        if fig:
            st.pyplot(fig)
            
        # Aprendizado cont√≠nuo
        st.subheader("üéì Sistema de Aprendizado Cont√≠nuo")
        study_plan = st.session_state.assistant.continuous_learning()
        st.markdown(study_plan)